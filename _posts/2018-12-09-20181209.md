---
layout: post
title : 基于HMM的歌声合成的音调自适应训练
cover : 
tags  : HTS HMM
mathjax: true
---

<h2>基于HMM的歌声合成的音调自适应训练</h2>

<h3>前言</h3>

基于隐马尔可夫模型（HMM）的歌唱语音合成的统计参数方法在过去几年中越来越受欢迎。该方法中的歌声的频谱，激励，颤音和持续时间同时都依赖于上下文的HMM建模，并且从HMM本身生成波形。基于HMM的歌唱语音合成系统在很大程度上基于性能训练数据，因为这些系统是“基于语料库的”。因此，对应于几乎不会出现在训练数据中的上下文$factors$的HMM不能很好地训练。由于生成的$F0$轨迹对合成歌声的主观质量有很大影响，因此应特别正确地覆盖音高。我们将“朗读者自适应训练（$speaker\ adaptive\ training$）”（SAT）的方法应用于“音调自适应训练（$pitch\ adaptive\ training$）”来获取音调数据。本文将对此展开讨论。

<h3>介绍</h3>

基于隐马尔可夫模型（HMM）的语音合成的统计参数方法在过去几年中越来越受欢迎。 在该方法中，从语音数据库设计会估计上下文的HMM，并且从HMM自身生成语音波形。 该框架使得可以在不必记录大型语音数据库的情况下对不同的语音特征，说话风格或情绪进行建模。 例如，自适应，插值和本征语音技术已应用于该系统，已证明可以修改语音特征。 通过应用基于HMM的方法也提出了一种歌声合成系统。

合成歌声的质量很大程度上取决于训练数据，因为基于HMM的歌声合成系统是“基于语料库的”。因此，对应于很少出现在训练数据中的上下文$factors$的HMM是不能很好地训练滴。尽管在基于HMM的歌唱合成系统中应该使用包括各种上下文$factors$的数据库，但是几乎不可能涵盖所有可能的上下文$factors$，因为歌唱声音涉及大量的上下文$factors$，例如：歌词，颤音，持续时间，音高等。音高应特别适当地覆盖，因为它对合成歌声的主观质量有很大的影响。 使用音调移位伪数据的技术是该问题的一种解决方案。

但是，有各种各样的歌唱信号等问题，例如大量的计算成本等。 虽然已经提出了数据级别的音调标准化，但仍然存在一些其他问题。例如数据和训练之间的不一致。 本文提出了基于HMM的歌声合成的音调自适应训练来克服这些问题。 从波形中提取的$log F0$序列与音符的音高之间的差异可以在所提出的训练中建模。

<h3>基于HMM的歌声合成系统</h3>

基于HMM的歌声合成系统与基于HMM的文本到语音合成系统非常相似。但是，它们之间存在明显差异。上图给出了基于HMM的歌声合成系统的概述。 它由培训和合成部分组成。频谱（例如，$mel-cepstral$系数），激发，从训练部分的歌声数据库中提取颤音和音高，然后用依赖于上下文的HMM对它们进行建模。 还估计了状态持续时间的依赖于上下文的模型。 包括要合成的歌词的任意给定的乐谱首先被转换为合成部分中的依赖于上下文的标签序列。 其次，根据标签序列，通过连接依赖于上下文的HMM来构造对应于歌曲的HMM。 第三，关于状态持续时间模型确定歌曲HMM的状态持续时间。 第四，通过算法生成频谱，激励和颤音参数以生成语音参数。 最后，通过使用Mel对数谱近似（MLSA）滤波器，直接从所生成的频谱，激励和颤音参数合成歌声。

<h3>基于HMM的歌声合成的音调自适应训练</h3>

基于HMM的语音合成系统在很大程度上依赖于训练数据的性能，因为这些系统是“基于语料库的”。因此，与训练数据中几乎没有出现的上下文$factors$相对应的HMM不能很好地训练。已经提出了用于设计语音数据库的算法，其考虑了上下文$factors$之间的平衡以解决该问题。包括各种上下文$factors$的数据库也应该用在基于HMM的歌声合成系统中。然而，数据必须是稀疏的，因为除了用于阅读语音合成的那些之外，歌唱声音还包括许多上下文$factors$，例如音调，节奏，音高，节拍和颤音。 由于生成的$F0$轨迹对合成歌声的主观质量有很大影响，因此应特别正确地覆盖音高。

使用音调移位伪数据的技术是解决该问题的一种方法。由于音调由$log F0$参数表示，因此可以通过在半色调中向上或向下移动$log F0$来容易地准备音调移位的伪数据。该技术使得可以增加$F0$训练数据的量而不必记录大量的歌声数据。通过复制相同的数据来添加频谱和颤音参数，假设它们没有受到少量音高变换的影响。通过添加音调移位的伪数据，训练数据量增加了三倍。因此，当使用基于最小描述长度的标准时，决策树的大小增加。因此，当决策树达到适当的大小时，应该停止上下文聚类。在基于HMM的歌声合成系统中使用MDL标准来确定何时应该停止分割节点。启发式权重用于控制决策树的大小。当适当调整决策树的大小时，合成语音的质量得到改善。

虽然使用音调移位伪数据的技术提高了合成语音的质量，但这种技术还有其他四个问题：

<ul>
<li>在特定音调范围中的特征不能被建模，因为音调上下文由于音调移位的伪数据而被混合。</li>
<li>在训练数据的音高范围之外合成歌声取决于音高变换的量。</li>
<li>间距移位的伪数据增加了HMM训练中的计算成本。</li>
<li>必须手动适当调整参数数量的节距变换和启发量控制的数量。</li>
</ul>

需要间距归一化技术来克服这四个问题。

数据级音调归一化是这些问题的一种解决方案，其中从波形提取的$log F0$序列与音符的音调之间的差异用于训练数据。 但是，还有两个问题：

<ul>
<li>需要音符水平的对齐来准备训练数据以计算差异。</li>
<li>如果在HMM训练中音符水平的对齐是固定的，则不能同时估计参数（频谱，激励，颤音或时长）。 但是，如果在HMM训练中没有修复它们，则无法很好地估计差异，因为数据和训练之间存在不一致。</li>
</ul>

本文提出了音高的模型级标准化来克服这些问题。 “朗读者自适应训练”的方法应用于“音调自适应训练”。上图比较了数据级和模型级音调标准化。 假设训练说话者的语音和平均语音之间的差异在SAT算法中表示为状态输出分布的均值向量的简单线性回归函数：

$$
\begin{aligned} \boldsymbol{\mu}_{i}^{(f)} &=\boldsymbol{W}_{i}^{(f)} \boldsymbol{\xi}_{i} \\ \boldsymbol{W}_{i}^{(f)} &=\left[\boldsymbol{\zeta}_{i}^{(f)}, \boldsymbol{\epsilon}_{i}^{(f)}\right] \\ \boldsymbol{\xi}_{i} &=\left[\boldsymbol{\mu}_{i}^{\top}, 1\right]^{\top} \end{aligned}
$$

其中$ \mu ^{\left( f\right) }_{i} $和$\xi _{i} $对应于训练说话者$i$的状态$f$的平均向量，一个变换矩阵，表示训练说话者f和平均语音之间的差异和平均语音的扩展平均向量。 SAT算法同时估计HMM的参数集和每个训练说话者的变换矩阵集，以便最大化可能性。

然而，状态$i$中$log F0$的静态特征的平均值$\hat{\mu}_{i}$在pitch自适应训练算法中被定义为

$$
\begin{aligned} \hat{\mu}_{i} &=\boldsymbol{W}_{i} \boldsymbol{\xi}_{i} \\ \boldsymbol{W}_{i} &=\left[1, b_{i}\right] \\ \boldsymbol{\xi}_{i} &=\left[\mu_{i}, 1\right]^{\top} \end{aligned}
$$

其中$\mu{i}$是从波形中提取的$log F0$与音符音高之间的差值的平均值。$b{i}$是包括状态$i$的音符的$log F0$。 由于变换矩阵由乐谱固定，因此音调自适应训练仅估计HMM的参数集。 因此，传统方法的所有问题都可以通过以下1-6的步骤来解决：

<ul>
<li>可以对特定音高范围中包括的特征进行建模，因为所有训练数据都具有正确的音高上下文。</li>
<li>从波形中提取的log F0与音符的音高之间的差异被建模。 因此，可以合成未出现在训练数据中的音高。</li>
<li>训练数据总量没有增加。计算成本也没有增加。</li>
<li>MDL标准可用于自动控制参数的数量。</li>
<li>由于音高的模型级标准化，HMM训练不需要对齐音符。</li>
<li>无需在HMM训练中修复校准。可以同时估计所有参数。 数据和训练之间没有不一致之处。</li>
</ul>

<h2>实验数据分析</h2>

进行主观实验以评估基于HMM的歌唱的音调自适应训练的性能歌声合成。

<h4>实验条件</h4>

使用了一位女歌手所唱的70首日本儿童歌曲（总共71.8分钟）。为48K采样率，并以5ms的频率加窗。 特征向量由频谱，激励和颤音参数组成。 频谱参数矢量由49个STRAIGHT mel-cepstral系数组成，包括零系数，由$Δ$和$Δ-Δ$系数组成。 激励参数矢量由$log F0$和它的$Δ$和$Δ-Δ$组成。 颤音参数矢量由幅度（分）和频率（Hz）和$Δ$与$Δ-Δ$组成。

用隐藏的半马尔可夫模型（HSMM）对”七个状态“（包括开始和结束空状态），从左到右，无跳过的结构进行建模。使用单个多变量高斯分布对频谱流进行建模。 激励和颤音流用多空间概率分布HSMM（MSD-HSMM）建模。每个模型的状态持续时间用五维多变量高斯分布建模。 基于决策树的上下文聚类技术分别应用于频谱，激励，颤音，状态持续时间和时间的分布。 MDL标准用于控制决策树的大小。 生成语音参数的算法用于生成频谱参数，该算法考虑了无静默的上下文相关全局方差（GV）。

评估基线方法，使用变速的传统方法和使用音调自适应训练的所提出的方法。 注意，为了公平，用于控制常规方法和所提出方法中使用的参数数量的启发式权重是基于基线方法手动调整的，因为MDL标准不能用于传统方法。 在传统方法中，音调偏移伪数据的范围是$±$半音。

<h3>文章引用</h3>

[0] Keiichiro Oura, Ayami Mase, Yoshihiko Nankaku, and Keiichi Tokuda"PITCH ADAPTIVE TRAINING FOR HMM-BASED SINGING VOICE SYNTHESIS"<br/>
[1] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura, “Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis,” in Proc. of Eurospeech, 1999, pp. 2347–2350.<br/>
[2] K. Oura, A. Mase, T. Yamada, S. Muto, Y. Nankaku, and K. Tokuda, “Recent development of the HMM-based singing voice synthesis system ―Sinsy,” in Proc. the 7th ISCA Tutorial and Research Workshop on Speech Synthe- sis, 2010, pp. 211–216.<br/>
[3] A. Mase, K. Oura, Y. Nankaku, and K. Tokuda, “HMM- based singing voice synthesis system using pitch-shifted pseudo training data,” in Proc. of Interspeech, 2010, pp. 845–848.
[4] K. Saino, M. Tachibana, and H. Kenmochi, “An HMM- based singing style modeling system for singing voice synthesizers,” in Proc. the 7th ISCA Tutorial and Re- search Workshop on Speech Synthesis, 2010, pp. 252– 257.<br/>
[5] Yao Qian, Hui Liang, and Flank K. Soong, “Generating natural F0 trajectory with additive trees,” in Proc. of In- terspeech, 2008, pp. 2126–2129.<br/>
[6] Heiga Zen and Norbert Brauns chweiler, “Context- dependent additive log F0 model for HMM-based speech synthesis,” in Proc. of Interspeech, 2009, pp. 2091–2094.<br/>
[7] A. Kuramatsu, K. Takeda, Y. Sagisaka, S. Katagiri, H. Kawabara, and K. Shikano, “ATR Japanese speech database as a tool of speech recognition and synthesis,” in Speech Communication, 1990, vol. 9, pp. 357–363.<br/>
[8] J. Yamagishi and T. Kobayashi, “Average-voice-based speech synthesis using HSMM-based speaker adaptation and adaptive training,” IEICE Trans. Inf. &amp; Syst., vol. E-90D, no. 2, pp. 533–543, 2007.<br/>
[9] K. Saino, H. Zen, Y. Nankaku, A. Lee, and K. Tokuda, “An HMM-based singing voice synthesis system,” in Proc. of ICSLP, 2006, pp. 1141–1144.