---
layout: post
title : HMM决策树聚类
cover : 
tags  : HTS HMM
mathjax: true
---

用于基于HMM的语音识别的模型训练的聚类方法大致可以分为两类：一是基于数据驱动的聚类方法，而另一个就是现今正广泛使用的基于决策树的聚类方法。决策树聚类的好处是：第一，它可以得到与基于数据驱动聚类方法相同的聚类性能，另一个好处更为重要，就是它可以为训练集未包含的但实际语流中又可能会出现的语音单元（称为不可见语音单元）提供一个较为可信的参数估计。

<h2>问题：</h2>

考虑到模型所处的上下文，一个发音会由于上下文音素的不同而产生不同的发音，我们在说话的时候，往往在某些发音还没有充分发出时候，就转入下一个音，也就是连音问题，因此要建立一个上下文相关的HMM。<br/>

虽然连续高斯分布可以比较好的描述观测概率，能够比较好的描述语音的变异性，但是由于连续语音中的协同发音现象会严重影响到一个模型的性能。发音器官某个位置受到前面和后面的发音影响，会导致同一个发音在不同的上下文关系中的特征，那么将会使类内方差增大，降低语音识别系统的一个性能。因此需要建立一个不同的上下文环境建立不同的音素模型。<br/>

如果一个单音素的模型序列为: <br/>
`sil g ow s t r ey t sil` <br/>
则使用的三元音素模型为： <br/>
`sil sil-g+ow g-ow+s ow-s+t s-t+r t-r+ey r-ey+t ey-t+sil sil` <br/>
引入三元音素模型将使模型的数量急剧增大，这样会导致每个训练模型的训练数据严重不足，无法得到可靠的参数估计，最终影响系统的识别性能。一般是采用状态捆绑的方法来实现参数共享。基于决策树的状态捆绑方法成为解决三元音素模型训练数据的不同方法之一。整个过程通过遍历一个二叉决策树来完成。 <br/>
开始的时候所有的以aw为基元的三元音素模型都位于根节点，然后根据语音学分类的问题判断各个模型应该划分到左子树和右子树，不断重复，直到达到叶子节点位置，处于同一叶子节点的模型当做同一类，并将其状态参数进行捆绑。<br/>

<h2>基本步骤：</h2>

（1）克隆HMM状态 <br/>
利用单音素训练集抄本，考虑词内上下文的关系，产生初始粗糙的三音素模型，每个三音素的参数直接拷贝的单音素参数。再初始的triphone HMM建立好之后，利用和训练单音素相同的嵌入式训练算法，进行模型训练，达到期望值收敛。<br/>

（2）绑定参数 <br/>
为了达到较好的识别效果，我们需要在模型的复杂度（模型参数的不同级别、参数的数量等等）和利用有限的语音训练数据来平衡模型。通过有限的训练数据去重估模型参数是不现实的。于是采用决策树聚类的 方法对三因素进行绑定。 <br/>
待分类的数据就是不同的三音素HMM集合，判断问题就是关于上下文的声学-语言学问题。每一个节点都是一个状态集合，并且具有一个产生某个具体观测数据的概率，根据这些问题的回答，状态集合中的状态可以被分到左节点和右节点。这些子节点又会产生一个新的概率，这两个子节点的概率之和会大于父节点的概率。决策树的分裂规则就是使得父节点和子节点的概率值差最大，停止规则是预先设定的阈值。 <br/>
决策树的根节点，将可能的上下文放在这个根节点上，为了得到某个最佳分裂，使用问题集合中的每一个问题进行一次分裂，并计算分裂之后的概率增加。若是这个概率增加超过了实现的阈值，而且这个节点相联系的训练样本数目超过了某个最小阈值，则最终从问题集合中选择使概率增加最大的问题作为该节点的最终分裂点，分裂之后产生两个子节点，对每个节点重复上述操作。<br/>

<h2>思考：</h2>

基于data-driven的聚类算法，如KNN，有个缺点，就是无法处理训练数据中没有出现过的triphone，基于决策树的聚类可以解决这个问题。

基于决策树的聚类过程中，把任意一个phone分裂成2类，其log 似然必然会增加，因为分裂后同样的个数的数据，其参数增加一倍，因此，选择不同的question对phone集合进行分裂，使得分裂后的log 似然增加最大。具体描述如下：

1）初始所有的状态都在一个类别中，作为tree的root节点；

2）找到一个question，使得分裂后的2个类别的log 似然增加最大，把当前的question作为当前tree节点的问题；

3）从上到家重复这个过程，直到分裂后，增加的log 似然小于某个给定的阈值；

4）最后，对不同父亲节点的两个叶子节点的类别成一个类，计算其减少的log 似然是否小于给定的阈值，如果小于这个阈值，则合并，否则不合并。

其中log似然计算公式如下：$s$表示HMM中的状态，F代表训练数据中的帧，$γ$代表占用数(state occupation counts)

$$
L(S)=\sum_{s \in S} \sum_{f \in F} \log \operatorname{Pr}\left(o_{f} ; \text {Distribution}\right) * \gamma_{s}\left(o_{f}\right)
$$

$$
L(S)=-\frac{1}{2}\left(\log \left[(2 \pi)^{n}\left|\sum S\right|\right]+n\right) \sum_{s \in S} \sum_{f \in F} \gamma_{s}\left(o_{f}\right)
$$

$$
\Delta L_{q}=L\left(S_{y}(q)\right)+L\left(S_{n}(q)\right)-L(S)
$$

实际中通过monophone训练triphone的过程综合了状态绑定和决策树聚类的过程，核心步骤如下所示：

1）用flat-start方式创建 monophone集合，每个状态的输出概率密度函数为单高斯概率密度，均值，方差为训练数据的全局均值和方差；

2）用EM算法迭代3-4次，对高斯参数重新进行估计；

3）对train data中出现的所有x-q+y音素，拷贝monophone q的状态的参数给triphone x-q+y，这样创建了上下文相关音素x-q+y，这步也是对triphone进行初始化的过程

4) 对这些triphone，用EM算法再次迭代，来高斯参数重新进行，同时状态j的占用数(state occupation counts)γ被计算出来

5) 把这些trephine用上述介绍的决策树聚类算法进行聚类。

6）最后对每个类增加高斯混合模型的component，并用EM重新估算高斯参数，直到节点个数到达给定的阈值。

以上就完成了monophone生成triphone的过程。

参考资料：

https://blog.csdn.net/xiaoding133/article/details/8491393<br/>
https://blog.csdn.net/quheDiegooo/article/details/61202901 <br/>